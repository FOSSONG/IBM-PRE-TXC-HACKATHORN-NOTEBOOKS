These notebooks contain prompting sessions for the Granite-13b-Chat-v2 model used in IBM Watsonx.ai prompt lab for the IBM pre-txc-hackathorn competition. Two separate prompt instructions were used and the model could distinguish between the prompt in the outputs.
The generative AI model in the IBM Watsonx.ai platform used in this challenge is the Granite-13b-Chat-v2 model. First, an IBM cloud account is created, and then we navigated to the prompt lab and chose the Granite-13b-Chat-v2 model. The model was then prompted based on our problem statement / research questions. The model was tested with zero-shot prompt, one-shot prompt and few-shot prompt. The model had the best performance when provided with many examples. Other models like the Granite-13b-instruct-v2, flan-ul2-20b, was also tested in the same way but the Granite-13b-Chat-v2 model had the best performance for these business use cases. We also played around with model parameters like repetition penalties, stopping sequences and number of tokens and got different results. To test the robustness and versatility of the Granite-13b-Chat-v2 model, we provided two separate prompts that are not related to each other and instructed the model to generate tokens specific to each prompt if a test data related to that prompt is inputted in the test section. We also used data in a semi-structured format (JSON). The performance of the model surpassed our expectations as the model could predict values and generate text with less than 10% error with zero-shot prompting, and less than 5% error with few-shot prompting. 
